{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2477a29951c9fed6f676e8681f876c026b317a6c"
   },
   "source": [
    "# Table of Content\n",
    "1. [Preparation](#preparation)\n",
    "2. [Dependency Graph](#dependency_graph)  \n",
    "    1. [Example 1](#dependency_graph_example1)\n",
    "    2. [Example 2](#dependency_graph_example2)\n",
    "3. [Feature Extraction](#feature_extraction)   \n",
    "4. [EDA](#eda)\n",
    "    1. [Tags Matching](#eda_tags_matching)\n",
    "    2. [Top Groups](#eda_top_groups)\n",
    "    3. [First Answer](#eda_first_answer)\n",
    "    4. [Word Count](#eda_wordcound)\n",
    "5. [Recommendation](#recommendation)\n",
    "    1. [Example 1](#recommender_example1)\n",
    "    2. [Example 2](#recommender_example2)\n",
    "6. [Topic Model (LDA)](#topic_model)\n",
    "    1. [Model](#lda_model)\n",
    "    2. [Topics](#lda_topics)\n",
    "    3. [Document-Topic Probabilities](#lda_doc_topic_prob)\n",
    "    4. [Example 1](#lda_example1)\n",
    "    5. [Example 2](#lda_example2)\n",
    "7. [Next steps](#next_steps)\n",
    "8. [Version control](#version_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "09bdf3f10f1df59055536781ff5a2a01a747ba10"
   },
   "source": [
    "# 1. Preparation <a id=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1e81c402070fff822cad82f5bbe29c7685034e9"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "6d4be46cd101250d3bbdeb85f8654c69e0e26a0a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-efc2bc132a63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('ner')\n",
    "#nlp.remove_pipe('tagger')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1d5a21aa975b2b1800c9d17eed2c33a2d4cef85"
   },
   "source": [
    "## Read CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "input_dir = '../input'\n",
    "print(os.listdir(input_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "professionals = pd.read_csv(os.path.join(input_dir, 'professionals.csv'))\n",
    "groups = pd.read_csv(os.path.join(input_dir, 'groups.csv'))\n",
    "comments = pd.read_csv(os.path.join(input_dir, 'comments.csv'))\n",
    "school_memberships = pd.read_csv(os.path.join(input_dir, 'school_memberships.csv'))\n",
    "tags = pd.read_csv(os.path.join(input_dir, 'tags.csv'))\n",
    "emails = pd.read_csv(os.path.join(input_dir, 'emails.csv'))\n",
    "group_memberships = pd.read_csv(os.path.join(input_dir, 'group_memberships.csv'))\n",
    "answers = pd.read_csv(os.path.join(input_dir, 'answers.csv'))\n",
    "students = pd.read_csv(os.path.join(input_dir, 'students.csv'))\n",
    "matches = pd.read_csv(os.path.join(input_dir, 'matches.csv'))\n",
    "questions = pd.read_csv(os.path.join(input_dir, 'questions.csv'))\n",
    "tag_users = pd.read_csv(os.path.join(input_dir, 'tag_users.csv'))\n",
    "tag_questions = pd.read_csv(os.path.join(input_dir, 'tag_questions.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6e7d123adb9929ad477099c0e7d6e635924a18b"
   },
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f8541c637620924140dad5ddc739a175067af3b"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "seed = 13\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c30e40157dcf4e3fa171b0b325e49975a48c9af"
   },
   "source": [
    "# 2. Dependency Graph <a id=\"dependency_graph\"></a>  \n",
    "![workflow_diagram](https://i.imgur.com/zzAo1JD.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7595015f701fbdd68e81b428b1c43aaa3d3efec"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4e0be924887454ef9324c2035fde01baed70286"
   },
   "outputs": [],
   "source": [
    "def plot_dependecy_graph(email_index=seed, plot_graph=True, print_report=False):\n",
    "    \"\"\" Merges all relevant data for a given email together and builds a dependency graph and report.\n",
    "\n",
    "        Actual missing: Group membership and School membership\n",
    "        \n",
    "        :param email_index: Index of the 'emails' dataframe (default: seed)\n",
    "        :param plot_graph: Boolean to plot the graph (default: True)\n",
    "        :param print_report: Boolean to print the text report (default: False)\n",
    "    \"\"\"  \n",
    "    email_id = emails.loc[email_index, 'emails_id'].values\n",
    "    # Merge the dataframes\n",
    "    graph_data = matches[matches['matches_email_id'].isin(email_id)]\n",
    "    graph_data = pd.merge(graph_data, questions, left_on='matches_question_id', right_on='questions_id', how='left')\n",
    "    graph_data = pd.merge(graph_data, answers, left_on='questions_id', right_on='answers_question_id', how='left')\n",
    "    graph_data = pd.merge(graph_data, tag_questions, left_on='questions_id', right_on='tag_questions_question_id', how='left')\n",
    "    graph_data = pd.merge(graph_data, tags, left_on='tag_questions_tag_id', right_on='tags_tag_id', how='left')\n",
    "    graph_data = pd.merge(graph_data, tag_users, left_on='questions_author_id', right_on='tag_users_user_id', how='left', suffixes=('', '_student'))\n",
    "    graph_data = pd.merge(graph_data, tags, left_on='tag_users_tag_id', right_on='tags_tag_id', how='left', suffixes=('', '_student'))\n",
    "    graph_data = pd.merge(graph_data, group_memberships, left_on='questions_author_id', right_on='group_memberships_user_id', how='left', suffixes=('', '_student'))\n",
    "    graph_data = pd.merge(graph_data, school_memberships, left_on='questions_author_id', right_on='school_memberships_user_id', how='left', suffixes=('', '_student'))\n",
    "    graph_data = pd.merge(graph_data, tag_users, left_on='answers_author_id', right_on='tag_users_user_id', how='left', suffixes=('', '_professional'))\n",
    "    graph_data = pd.merge(graph_data, tags, left_on='tag_users_tag_id_professional', right_on='tags_tag_id', how='left', suffixes=('', '_professional'))\n",
    "    graph_data = pd.merge(graph_data, group_memberships, left_on='answers_author_id', right_on='group_memberships_user_id', how='left', suffixes=('', '_professional'))\n",
    "    graph_data = pd.merge(graph_data, school_memberships, left_on='answers_author_id', right_on='school_memberships_user_id', how='left', suffixes=('', '_professional'))    \n",
    "    \n",
    "    if plot_graph:\n",
    "        plt.figure(figsize=(15, 15)) \n",
    "        G = nx.Graph()\n",
    "        node_color = []\n",
    "        # Nodes\n",
    "        df_nodes = pd.DataFrame({'node':['matches_email_id', 'questions_id', 'questions_author_id', 'answers_id', 'answers_author_id', 'group_memberships_group_id', \n",
    "                                'group_memberships_group_id_professional', 'school_memberships_school_id', 'school_memberships_school_id_professional',\n",
    "                                'tags_tag_name', 'tags_tag_name_student', 'tags_tag_name_professional'],\n",
    "                         'color':['grey', 'blue', 'green', 'red', 'cyan', 'orange', 'orange', 'purple', 'purple', 'yellow', 'yellow', 'yellow']})\n",
    "        for index, row in df_nodes.iterrows():\n",
    "            G.add_nodes_from(graph_data[row['node']].dropna().unique())\n",
    "            node_color +=([row['color']]*len(graph_data[row['node']].dropna().unique()))\n",
    "        # Edges \n",
    "        df_edges = pd.DataFrame({'source':['matches_email_id', 'questions_id', 'questions_id', 'answers_id', 'questions_id',\n",
    "                                          'questions_author_id', 'answers_author_id', 'questions_author_id',\n",
    "                                          'answers_author_id', 'answers_author_id', 'answers_author_id'],\n",
    "                                'target':['questions_id', 'questions_author_id', 'answers_id', 'answers_author_id', 'tags_tag_name',\n",
    "                                         'tags_tag_name_student', 'tags_tag_name_professional', 'group_memberships_group_id',\n",
    "                                         'group_memberships_group_id_professional', 'school_memberships_school_id', 'school_memberships_school_id_professional']})\n",
    "        for index, row in df_edges.iterrows():\n",
    "            G.add_edges_from({tuple(row) for i,row in graph_data[[row['source'], row['target']]].dropna().iterrows()})\n",
    "\n",
    "        nx.draw_networkx(G, with_labels=True, node_color=node_color, font_size=8, node_size=900)\n",
    "        plt.title('Dependency graph for email {}'.format(email_id))\n",
    "        plt.axis('off')\n",
    "\n",
    "        legend_email = mpatches.Patch(color='grey', label='Email')\n",
    "        legend_question = mpatches.Patch(color='blue', label='Question')\n",
    "        legend_student = mpatches.Patch(color='green', label='Student')\n",
    "        legend_answer = mpatches.Patch(color='red', label='Answer')\n",
    "        legend_professional = mpatches.Patch(color='cyan', label='Professional')\n",
    "        legend_tag = mpatches.Patch(color='yellow', label='Tag')\n",
    "        legend_group = mpatches.Patch(color='orange', label='Group')\n",
    "        legend_school = mpatches.Patch(color='purple', label='School')\n",
    "        plt.legend(handles=[legend_email, legend_question, legend_student, legend_answer, legend_professional, legend_tag, legend_group, legend_school])\n",
    "        plt.show()\n",
    "    \n",
    "    if print_report:\n",
    "        print('Email ID: {}'.format(email_id))\n",
    "        print('Questions: {}'.format(len(graph_data['questions_id'].unique())))\n",
    "        for question in graph_data['questions_id'].unique():\n",
    "            date = graph_data[graph_data['questions_id'] == question]['questions_date_added'].dropna().unique()[0]\n",
    "            author = graph_data[graph_data['questions_id'] == question]['questions_author_id'].dropna().unique()[0]\n",
    "            question_tags = graph_data[graph_data['questions_id'] == question]['tags_tag_name'].dropna().unique()\n",
    "            author_tags = graph_data[graph_data['questions_id'] == question]['tags_tag_name_student'].dropna().unique()\n",
    "            author_groups = graph_data[graph_data['questions_id'] == question]['group_memberships_group_id'].dropna().unique()\n",
    "            author_school = graph_data[graph_data['questions_id'] == question]['school_memberships_school_id'].dropna().apply('{0:.0f}'.format).unique()\n",
    "            question_answers = graph_data[graph_data['questions_id'] == question]['answers_id'].dropna().unique()\n",
    "            print('  \\033[44mQuestion {}\\033[0m from \\033[42mstudent {}\\033[0m on {}'.format(question, author, date))\n",
    "            print('    Question Tags: {}'.format(', '.join(question_tags)))\n",
    "            print('    Student Tags: {}'.format(', '.join(author_tags)))\n",
    "            print('    Student Groups: {}'.format(', '.join(author_groups)))\n",
    "            print('    Student Schools: {}'.format(', '.join(author_school)))\n",
    "            print('    Answers: {}'.format(len(question_answers)))\n",
    "            for question_answer in question_answers:\n",
    "                date = graph_data[graph_data['answers_id'] == question_answer]['answers_date_added'].dropna().unique()[0]\n",
    "                author = graph_data[graph_data['answers_id'] == question_answer]['answers_author_id'].dropna().unique()[0]\n",
    "                author_tags = graph_data[graph_data['answers_id'] == question_answer]['tags_tag_name_professional'].dropna().unique()\n",
    "                author_groups = graph_data[graph_data['answers_id'] == question_answer]['group_memberships_group_id_professional'].dropna().unique()\n",
    "                author_school = graph_data[graph_data['answers_id'] == question_answer]['school_memberships_school_id_professional'].dropna().apply('{0:.0f}'.format).unique()\n",
    "                print('      \\033[41mAnswer {}\\033[0m from \\033[46mprofessional {}\\033[0m on {}'.format(question_answer, author, date))\n",
    "                print('        Professional Tags: {}'.format(', '.join(author_tags)))\n",
    "                print('        Professional Groups: {}'.format(', '.join(author_groups)))\n",
    "                print('        Professional Schools: {}'.format(', '.join(author_school)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f470ea2a921db552c00464308bc25e7935d4cbf"
   },
   "source": [
    "## Example 1  <a id=\"dependency_graph_example1\"></a> \n",
    "In example 1 we have <span style=\"background-color:gray\">one email</span> with <span style=\"background-color:blue\">two questions</span> with a few <span style=\"background-color:yellow\">questions tags</span>. There are <span style=\"background-color:red\">several answers</span> for the question. The <span style=\"background-color:green\">students</span> from the questions haven't any <span style=\"background-color:orange\">group</span> or <span style=\"background-color:purple\">school</span> membership. The <span style=\"background-color:cyan\">professionals</span> have more motivation to subscribe some <span style=\"background-color:yellow\">tags</span> and specify there <span style=\"background-color:purple\">school</span> membership. But only <span style=\"background-color:cyan\">two professionals</span> have joined a <span style=\"background-color:orange\">group</span>.<span style=\"background-color:blue\"></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb4832314f033b6c1c2d173e8920188c0a1b4e07"
   },
   "outputs": [],
   "source": [
    "plot_dependecy_graph(email_index=[seed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c680d875340c5e81eea87f8ecdb5d747593154f9"
   },
   "source": [
    "## Example 2 <a id=\"dependency_graph_example2\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "062cef3c409cc0fc75220ff69856fd7e7867e594"
   },
   "outputs": [],
   "source": [
    "plot_dependecy_graph(email_index=[seed*2], print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3eae14cbe5d63c41176b6af25ab66dd57943d16"
   },
   "source": [
    "# 3. Features extraction <a id=\"feature_extraction\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be3907611ab722da9836e599c5f46155a537844d"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b22dc7ef369701cd50a599d4d7898ad0de9acbb"
   },
   "outputs": [],
   "source": [
    "# Spacy Tokenfilter for part-of-speech tagging\n",
    "token_pos = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'INTJ', 'X']\n",
    "\n",
    "# The data export was from 1. February 2019. For Production use datetime.now()\n",
    "actual_date = datetime.datetime(2019, 2 ,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "381f5a983596fa8a3ab585a168b6b9b3aaac09b7"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfc55238256ffcb2125573fcecff97a0b5c4ef0f"
   },
   "outputs": [],
   "source": [
    "def nlp_preprocessing(data):\n",
    "    \"\"\" Use NLP to transform the text corpus to cleaned sentences and word tokens\n",
    "\n",
    "    \"\"\"    \n",
    "    def token_filter(token):\n",
    "        \"\"\" Keep tokens who are alphapetic, in the pos (part-of-speech) list and not in stop list\n",
    "\n",
    "        \"\"\"    \n",
    "        return not token.is_stop and token.is_alpha and token.pos_ in token_pos\n",
    "    \n",
    "    processed_tokens = []\n",
    "    data_pipe = nlp.pipe(data)\n",
    "    for doc in data_pipe:\n",
    "        filtered_tokens = [token.lemma_.lower() for token in doc if token_filter(token)]\n",
    "        processed_tokens.append(filtered_tokens)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "86ca1667419efbd80c2e81940fc98a9c5d779c10"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f38d504ab3c3bb63c9592760203d36f37e476ed1"
   },
   "outputs": [],
   "source": [
    "# Transform datatypes\n",
    "questions['questions_date_added'] = pd.to_datetime(questions['questions_date_added'])\n",
    "answers['answers_date_added'] = pd.to_datetime(answers['answers_date_added'])\n",
    "professionals['professionals_date_joined'] = pd.to_datetime(professionals['professionals_date_joined'])\n",
    "students['students_date_joined'] = pd.to_datetime(students['students_date_joined'])\n",
    "\n",
    "### Questions\n",
    "# Merge Question Title and Body\n",
    "questions['questions_full_text'] = questions['questions_title'] +'\\r\\n\\r\\n'+ questions['questions_body']\n",
    "# Count of answers\n",
    "temp = answers.groupby('answers_question_id').size()\n",
    "questions['questions_answers_count'] = pd.merge(questions, pd.DataFrame(temp.rename('count')), left_on='questions_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n",
    "# First answer for questions\n",
    "temp = answers[['answers_question_id', 'answers_date_added']].groupby('answers_question_id').min()\n",
    "questions['questions_first_answers'] = pd.merge(questions, pd.DataFrame(temp), left_on='questions_id', right_index=True, how='left')['answers_date_added']\n",
    "# Last answer for questions\n",
    "temp = answers[['answers_question_id', 'answers_date_added']].groupby('answers_question_id').max()\n",
    "questions['questions_last_answers'] = pd.merge(questions, pd.DataFrame(temp), left_on='questions_id', right_index=True, how='left')['answers_date_added']\n",
    "\n",
    "### Answers\n",
    "# Days required to answer the question\n",
    "temp = pd.merge(questions, answers, left_on='questions_id', right_on='answers_question_id')\n",
    "answers['time_delta_answer'] = (temp['answers_date_added'] - temp['questions_date_added'])\n",
    "\n",
    "### Professionals\n",
    "# Time since joining\n",
    "#professionals['professionals_time_delta_joined'] = actual_date - professionals['professionals_date_joined']\n",
    "# Number of answers\n",
    "temp = answers.groupby('answers_author_id').size()\n",
    "professionals['professionals_answers_count'] = pd.merge(professionals, pd.DataFrame(temp.rename('count')), left_on='professionals_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n",
    "# Last activity (Answer)\n",
    "temp = answers.groupby('answers_author_id')['answers_date_added'].max()\n",
    "professionals['date_last_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_answer')), left_on='professionals_id', right_index=True, how='left')['last_answer']\n",
    "# Avg answers per week\n",
    "#professionals['avg_answers_week'] = (professionals['answers_count'] / ((professionals['date_last_answer'] - professionals['professionals_date_joined']).dt.days).apply(lambda x: np.ceil(x/7) if x > 0 else 1))\n",
    "\n",
    "### Students\n",
    "# Time since joining\n",
    "#students['students_time_delta_joined'] = actual_date - students['students_date_joined']\n",
    "# Number of answers\n",
    "temp = questions.groupby('questions_author_id').size()\n",
    "students['students_questions_count'] = pd.merge(students, pd.DataFrame(temp.rename('count')), left_on='students_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n",
    "# Last activity (Question)\n",
    "temp = questions.groupby('questions_author_id')['questions_date_added'].max()\n",
    "students['date_last_question'] = pd.merge(students, pd.DataFrame(temp.rename('last_question')), left_on='students_id', right_index=True, how='left')['last_question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b03f0852210e8ae7208b35167d8ec7350c9332c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get NLP Tokens\n",
    "questions['nlp_tokens'] = nlp_preprocessing(questions['questions_full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4624b3acfa13e3d1c66b2ece9c87fade4649f04c"
   },
   "source": [
    "# 3. EDA <a id=\"eda\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a02b159ea4f05e445ffd7369e2a6199eab8c710"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a85afd311a204976baa00a8efe3cb69c778a5ee"
   },
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    \"\"\" Count the words in the text\n",
    "    \n",
    "    \"\"\"\n",
    "    result = text.split()\n",
    "    result = len(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb8cc07cdbba47d65e2b1682f67549e2c1ca050b"
   },
   "outputs": [],
   "source": [
    "def plot_tags_matching():\n",
    "    students_tags = tag_users[tag_users['tag_users_user_id'].isin(students['students_id'])]\n",
    "    students_tags = pd.merge(students_tags, tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\n",
    "    students_tags['user_type'] = 'student'\n",
    "    professionals_tags = tag_users[tag_users['tag_users_user_id'].isin(professionals['professionals_id'])]\n",
    "    professionals_tags = pd.merge(professionals_tags, tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\n",
    "    professionals_tags['user_type'] = 'professional'\n",
    "    questions_tags = tag_questions\n",
    "    questions_tags = pd.merge(questions_tags, tags, left_on='tag_questions_tag_id', right_on='tags_tag_id')\n",
    "    questions_tags['user_type'] = 'question'\n",
    "    plt_data = pd.concat([students_tags, professionals_tags, questions_tags])\n",
    "\n",
    "    plt_data = plt_data[['tags_tag_name', 'user_type']].pivot_table(index='tags_tag_name', columns='user_type', aggfunc=len, fill_value=0)\n",
    "    plt_data['professional'] = plt_data['professional'] / professionals.shape[0]\n",
    "    plt_data['student'] = plt_data['student'] / students.shape[0]\n",
    "    plt_data['question'] = plt_data['question'] / questions.shape[0]\n",
    "    plt_data['sum'] = (plt_data['professional'] + plt_data['student'] + plt_data['question'])\n",
    "    plt_data = plt_data.sort_values(by='sum', ascending=False).drop(['sum'], axis=1).head(100)\n",
    "\n",
    "    # Bubble chart\n",
    "    fig, ax = plt.subplots(facecolor='w',figsize=(15, 15))\n",
    "    ax.set_xlabel('Professionals')\n",
    "    ax.set_ylabel('Students')\n",
    "    ax.set_title('Tags Matching')\n",
    "    ax.set_xlim([0, max(plt_data['professional'])+0.001])\n",
    "    ax.set_ylim([0, max(plt_data['student'])+0.001])\n",
    "    import matplotlib.ticker as mtick\n",
    "    ax.xaxis.set_major_formatter(mtick.FuncFormatter(\"{:.2%}\".format))\n",
    "    ax.yaxis.set_major_formatter(mtick.FuncFormatter(\"{:.2%}\".format))\n",
    "    ax.grid(True)\n",
    "    i = 0\n",
    "    for key, row in plt_data.iterrows():\n",
    "        ax.scatter(row['professional'], row['student'], s=row['question']*10**4, alpha=.5)\n",
    "        if i < 25:\n",
    "            ax.annotate('{}: {:.2%}'.format(key, row['question']), xy=(row['professional'], row['student']))\n",
    "        i += 1\n",
    "    plt.show()\n",
    "    \n",
    "    # Wordcloud\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    wordloud_values = ['student', 'professional', 'question']\n",
    "    axisNum = 1\n",
    "    for wordcloud_value in wordloud_values:\n",
    "        wordcloud = WordCloud(margin=0, max_words=20, random_state=seed).generate_from_frequencies(plt_data[wordcloud_value])\n",
    "        ax = plt.subplot(1, 3, axisNum)\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(wordcloud_value)\n",
    "        plt.axis(\"off\")\n",
    "        axisNum += 1\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66f9998d991bafbb0f37956be0040d58d24372bf"
   },
   "outputs": [],
   "source": [
    "def plot_top_groups():\n",
    "    plt_data = group_memberships.groupby('group_memberships_group_id').size().sort_values(ascending=False)\n",
    "    plt_data.plot(kind='bar', figsize=(15, 5), color='green')\n",
    "    plt_data.plot(lw=3, color='green')\n",
    "    plt.xticks(range(len(plt_data)), [])\n",
    "    plt.xlabel('Groups')\n",
    "    plt.ylabel('Members')\n",
    "    plt.title('Top Groups')\n",
    "    plt.show()\n",
    "    print(plt_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01a386f8cb0b81866346e9a4b6d2cb502dbbf493"
   },
   "outputs": [],
   "source": [
    "def plot_first_answer():\n",
    "    plt_data = (questions['questions_first_answers'] - questions['questions_date_added']).dt.days.rename(\"First Answer\")\n",
    "    print(plt_data.describe())\n",
    "    plt_data.plot(kind='box', showfliers=False, grid=True, vert=False, figsize=(15, 5))\n",
    "    plt.xlabel('Days')\n",
    "    plt.title('Time for first answer')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b4773f5239a97a71185ddddd36b29deca2d7ba5"
   },
   "outputs": [],
   "source": [
    "def plot_wordcount_questions():\n",
    "    plt_data_title = questions['questions_title'].apply(word_count).rename(\"Title\")\n",
    "    plt_data_body = questions['questions_body'].apply(word_count).rename(\"Body\")\n",
    "    plt_data_fulltext = questions['questions_full_text'].apply(word_count).rename(\"Full Text\")\n",
    "    plt_data = pd.DataFrame([plt_data_title, plt_data_body, plt_data_fulltext]).T\n",
    "    print(plt_data.describe())\n",
    "    plt_data.plot(kind='box', showfliers=False, vert=False, figsize=(15, 5), grid=True)\n",
    "    plt.xticks(range(0, 130, 10))\n",
    "    plt.xlabel('Words')\n",
    "    plt.title('Word count (Questions)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88c2e2a5860c0c4878bba595cbfd3b70bb3b4024"
   },
   "outputs": [],
   "source": [
    "def plot_wordcount_answers():\n",
    "    plt_data = answers['answers_body'].astype(str).apply(word_count).rename(\"Answers body\")\n",
    "    print(plt_data.describe())\n",
    "    plt_data.plot(kind='box', showfliers=False, vert=False, figsize=(15, 5), grid=True)\n",
    "    plt.xticks(range(0, 400, 50))\n",
    "    plt.xlabel('Words')\n",
    "    plt.title('Word count (Answers)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2e8036d7156d32e81e8faa43dbd2b169c3817d8"
   },
   "outputs": [],
   "source": [
    "def plot_wordcount():\n",
    "    plt_data_questions = questions['questions_full_text'].apply(word_count).rename(\"Questions\")\n",
    "    plt_data_answers = answers['answers_body'].astype(str).apply(word_count).rename(\"Answers\")\n",
    "    plt_data = pd.DataFrame([plt_data_questions, plt_data_answers]).T\n",
    "    print(plt_data.describe())\n",
    "    plt_data.plot(kind='box', showfliers=False, vert=False, figsize=(15, 5), grid=True)\n",
    "    plt.xticks(range(0, 400, 10))\n",
    "    plt.xlabel('Words')\n",
    "    plt.title('Word count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "38dc013010142d51d6c35b1eb3541301dc182a9d"
   },
   "source": [
    "## Tags matching <a id=\"eda_tags_matching\"></a> \n",
    "The size of the bubbles depends on how many questions the tag is used. The x-axis is how many professionals have subscribe the tag and the y-axis is how many students have subscribe the tag.  \n",
    "The top tag for professionals ist *telecommunications* on the right site with about 11% but the tag doesn't appear in many questions or students subscribtion.  \n",
    "The top tags for questions is *college* with 15.6% and *carrer* with 6.5%. The other top tags are carrer specific (*medicine, engineering, business, ...*).  \n",
    "The top tag for students is *college* but only 1.5% of the students have subscribe this tag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec9c393467a825686fe813daf0d6737048b9949f"
   },
   "outputs": [],
   "source": [
    "plot_tags_matching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1bf68605f4e77c95b1a4628d3b28bec4bac916d"
   },
   "source": [
    "## Top Groups  <a id=\"eda_top_groups\"></a>\n",
    "There are only two groups with more then 100 members. Related on the size of students and professionals, groups actual are only used by a small size of members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d258d4faec3eed7071c8da00f6e25480ec2af03b"
   },
   "outputs": [],
   "source": [
    "plot_top_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb576dc5e80d47966cc8ec3ec252df77d351d2f2"
   },
   "source": [
    "## Time for first answer  <a id=\"eda_first_answer\"></a>\n",
    "The most questions get the answer in the first days.  \n",
    "There are some outliers (e.q. the maximum with 1897 days) that are removed for the box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afd5a7cfaa1c5eb6e3e9b4242f4c547e37f791e2"
   },
   "outputs": [],
   "source": [
    "plot_first_answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6d4e7f060204eafa95b5b4eec7d9fe87da26aa2"
   },
   "source": [
    "## Word count <a id=\"eda_wordcound\"></a>\n",
    "Here we can see how many words are used for the questions and answers.  \n",
    "The professionals write very detailed answers for the students questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0e10c3d4ca8e10fe10bedc14389a484fd4a6179"
   },
   "outputs": [],
   "source": [
    "plot_wordcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d616e7970cb60c43aef6217b0264ec1507ae7b6"
   },
   "source": [
    "# 5. Recommendation <a id=\"recommendation\"></a>  \n",
    "With the preprocessed data I build a tf-idf corpus and can use this, to calculate the (cosine) similarity between a new question and the given questions.  \n",
    "Here are the detailed steps:  \n",
    "1. Use NLP on the Questions corpus.  \n",
    "    a. Use part-of-speech tagging to filter words.  \n",
    "    b. Calculate the tf-idf for a better Information Retrieval.  \n",
    "2. Use NLP on the Query text.  \n",
    "    a. Use part-of-speech tagging to filter words.  \n",
    "    b. Calculate the tf-idf for a better Information Retrieval.  \n",
    "3. Use the cosine similiarty to get similiar questions for the query text.  \n",
    "4. Get the answers and professionals for the similar questions.  \n",
    "5. Make a recommendation to fit the best professionals to answer the new question.  \n",
    "\n",
    "I use the similar questions and the professionals who answered the question to calculate a *recommendation score*. On the basis of this, professionals will be recommended who have already answered many similar questions which have not been answered by many others. The first draft of this formula is as follows  \n",
    "$Professional_{score} = \\sum\\limits_{q}^{Q}(q_{sim}*\\dfrac{1}{q_{answers}}*p_{answers})$  \n",
    "$Q$ = Similar  questions  answered  by  professional  p  \n",
    "$q_{sim}$ = Similarity of the question q  \n",
    "$q_{answers}$ = Total answers for question q  \n",
    "$p_{answers}$ = Total answers of professional p\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "748fb724c1017c58a471fe84d100d9e002a2104b"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24c22356a4d59ae213915b74f4141b565f9e5bab"
   },
   "outputs": [],
   "source": [
    "def get_similar_docs(corpus, query_text, threshold=0.0, top=5):\n",
    "    \"\"\" Calculates the tfidf of the corpus and returns similiar questions, matching the query text.\n",
    "\n",
    "    \"\"\"  \n",
    "    #nlp_corpus = [' '.join(x) for x in nlp_preprocessing(corpus)]\n",
    "    nlp_corpus = [' '.join(x) for x in questions['nlp_tokens']]\n",
    "    nlp_text = [' '.join(nlp_preprocessing([query_text])[0])]\n",
    "    vectorizer = TfidfVectorizer(lowercase = True, stop_words = 'english')\n",
    "    vectorizer.fit(nlp_corpus)\n",
    "    corpus_tfidf = vectorizer.transform(nlp_corpus)\n",
    "    \n",
    "    text_tfidf = vectorizer.transform(nlp_text)\n",
    "    sim = cosine_similarity(corpus_tfidf, text_tfidf)\n",
    "    sim_idx = (sim >= threshold).nonzero()[0]\n",
    "    result = pd.DataFrame({'similarity':sim[sim_idx].reshape(-1,),\n",
    "                          'text':corpus[sim_idx]},\n",
    "                          index=sim_idx)\n",
    "    result = result.sort_values(by=['similarity'], ascending=False).head(top)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da4aa6676852abbaf59f0e82b0a707032068d6a3"
   },
   "outputs": [],
   "source": [
    "def get_questions_answers(sim_questions):\n",
    "    \"\"\" Merges the questions with the corresponding answers\n",
    "\n",
    "    \"\"\"  \n",
    "    sim_question_answers = pd.merge(sim_questions, questions, left_index=True, right_index=True)\n",
    "    sim_question_answers = pd.merge(sim_question_answers, answers, left_on='questions_id', right_on='answers_question_id')\n",
    "    sim_question_answers = sim_question_answers[['questions_id', 'similarity', 'questions_title', 'questions_body', 'answers_body']]\n",
    "    return sim_question_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67e6ae0e687a8958f08433e4e67fe09a7ab20fb6"
   },
   "outputs": [],
   "source": [
    "def get_recommendation(sim_questions, plot_graph=True, print_report=True, top_n=5):\n",
    "    \"\"\" Get the top recommended professionals based on questions\n",
    "\n",
    "    \"\"\"    \n",
    "    df = pd.merge(sim_questions, questions, left_index=True, right_index=True)\n",
    "    df = pd.merge(df, answers, left_on='questions_id', right_on='answers_question_id')\n",
    "    df = df[['questions_id', 'similarity', 'answers_author_id']]\n",
    "    plot_data = pd.DataFrame(columns=['source', 'target', 'value'])\n",
    "    plot_data = plot_data.append(pd.DataFrame({'source':['question'] * len(df['questions_id'].drop_duplicates()),\n",
    "                                                   'target':df['questions_id'].drop_duplicates(),\n",
    "                                                  'value':df['similarity'].drop_duplicates()}), ignore_index=True)\n",
    "    temp_values = df['similarity']/df['questions_id'].apply(lambda x: df.groupby('questions_id').size()[x])\n",
    "    temp_values = temp_values * df['answers_author_id'].apply(lambda x: df.groupby('answers_author_id').size()[x])\n",
    "    plot_data = plot_data.append(pd.DataFrame({'source':df['questions_id'],\n",
    "                                                   'target':df['answers_author_id'],\n",
    "                                                  'value':temp_values}), ignore_index=True)\n",
    "\n",
    "    if plot_graph:\n",
    "        labels = plot_data['source'].append(plot_data['target']).unique()\n",
    "        sources = plot_data['source'].apply(lambda x: labels.tolist().index(x))\n",
    "        targets = plot_data['target'].apply(lambda x: labels.tolist().index(x))\n",
    "        values = plot_data['value']*100\n",
    "        \n",
    "        data = dict(\n",
    "            type='sankey',\n",
    "            node = dict(\n",
    "              label = labels,\n",
    "            ),\n",
    "            link = dict(\n",
    "              source = sources,\n",
    "              target = targets,\n",
    "              value = values\n",
    "          ))\n",
    "        layout =  dict(\n",
    "            title = \"Recommendation (Question -> Similar Questions -> Professionals)\"\n",
    "        )\n",
    "\n",
    "        fig = dict(data=[data], layout=layout)\n",
    "        py.iplot(fig, validate=False)\n",
    "    \n",
    "    if print_report:\n",
    "        top_professionals = plot_data[~plot_data['target'].isin(plot_data['source'])][['target', 'value']].groupby('target').sum()\n",
    "        top_professionals = top_professionals.reset_index().sort_values('value', ascending = False).head(top_n)\n",
    "        top_professionals.columns = ['professional', 'recommendation_score']\n",
    "        print(top_professionals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44cd0775fc6edb552e97b1a3c75baa6bc2bab6b1"
   },
   "source": [
    "## Example 1  <a id=\"recommender_example1\"></a> \n",
    "I use a already existing question to get a recommendation. It's a question about the process of becoming a lawyer and how hard it will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3bf8644af977e2957e757923158de8021c246d83"
   },
   "outputs": [],
   "source": [
    "sim_corpus = questions['questions_full_text']\n",
    "sim_text = sim_corpus[seed]\n",
    "print('Example 1 Question:\\n', sim_text)\n",
    "sim_questions = get_similar_docs(sim_corpus, sim_text, top=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35c19e3b411406894e10b9e1daa237ac533b0b9a"
   },
   "source": [
    "**Similar Questions:  **  \n",
    "In this example the first recommendation is the question itself.  \n",
    "But a look on the other recommendation seems to be a good match too. They are about *lawyer* and how *hard* it will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a515bef34bc323eb6ab88fccbf88a7828046e67"
   },
   "outputs": [],
   "source": [
    "sim_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0069e530fb8d4fb014d394476b67343876795b0c"
   },
   "source": [
    "**Answers to similar questions**  \n",
    "Now I can merge the recommended questions with the answers for these questions. This can be used to give the student who asked the question a first recommendation of his question. Maybe these answers are already an answer to his question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3983c59ab8010c4a1da331b3cfb4ba67905442ea",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_questions_answers(sim_questions).head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed49e33489786cf3038bb3b088b077ce7ef83768"
   },
   "source": [
    "**Recommended Professionals**  \n",
    "The left is the new question. In the middle are the top similar questions. The width of the line shows the similarity.  \n",
    "On the right are the professionals who have answered the similar questions from the middle. Here the width of the line is a recommendation score.  \n",
    "Professionals with a big box have a high score and should be recommended.  \n",
    "\n",
    "The Professional **c5c2ca95fcd3463a8852b8bc9d636313** has the highest score with 2.27. This is because he answered three questions with a high similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70cd0c6dedfcd71f0c47dfe81d547bdbb036246f"
   },
   "outputs": [],
   "source": [
    "get_recommendation(sim_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37b7481d137de7300591d6b451f855eed7f25218"
   },
   "source": [
    "## Example 2  <a id=\"recommender_example2\"></a> \n",
    "Example 2 use a new defined question about the carrer as a data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "307c556cddee20bfc8eea53ee854dc3d2426e1fd"
   },
   "outputs": [],
   "source": [
    "query_text = 'I will finish my college next year and would like to start a career as a data scientist. \\n'\\\n",
    "            +'What is the best way to become a good data scientist? #data-science'\n",
    "print('Example 2 Question:\\n', query_text)\n",
    "sim_questions = get_similar_docs(sim_corpus, query_text, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a427117c0b76d9e3c246c4ba5e3d35729f09bd9"
   },
   "source": [
    "**Similar Questions:  **  \n",
    "The recommended questions are also about the carrer and preparation of become a data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bbb41eca9ee678e9915059231aee52a36faa355e"
   },
   "outputs": [],
   "source": [
    "sim_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52f0cdc2c7e8df24cf683cf0f2811c9c523d1f95"
   },
   "source": [
    "**Answers to similar questions**  \n",
    "Here we have several answers to the recommended similar questions and can use this to forward the question to a professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f4888f57cd5ac700e7305180c5078b53d2c8e79"
   },
   "outputs": [],
   "source": [
    "get_questions_answers(sim_questions).head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed565266b26d9915b56361c0e98c2eddd2d5a830"
   },
   "source": [
    "**Recommended Professionals**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "108ec6effaca5756b3285b29afb656817e643009"
   },
   "outputs": [],
   "source": [
    "get_recommendation(sim_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56c74acb932eecf777a377b0f44483384e4da7ee"
   },
   "source": [
    "# 6. Topic Model (LDA) <a id=\"lda\"></a>  \n",
    "In this section I will implement a LDA Model to get topic probabilities for the questions. We can use this to see how topics are distributed across questions and which words characterize them.  \n",
    "New questions can be allocated to topics and forwarded to professional who are familiar with these topics.\n",
    "\n",
    "1. Use NLP on the Questions corpus.  \n",
    "    a. Use part-of-speech tagging to filter words.  \n",
    "    b. Filter extrem values from corpus.  \n",
    "    c. Calculate the tf-idf. \n",
    "2. Train a LDA Model.  \n",
    "3. Give the topics names.\n",
    "3. Get the topic probability of a query text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b77a0e8cc9215d0f019847f2990b28ba7465e1b"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea3ae0f16c1ea1b7c1350685f8011c597242509c"
   },
   "outputs": [],
   "source": [
    "# Gensim Dictionary\n",
    "extremes_no_below = 10\n",
    "extremes_no_above = 0.6\n",
    "extremes_keep_n = 8000\n",
    "\n",
    "# LDA\n",
    "num_topics = 18\n",
    "passes = 20\n",
    "chunksize = 1000\n",
    "alpha = 1/50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ea6ca7808f15a1c59df417acc998323117820ba"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "832000e03ca48de376e76633d0071abd61e523f3"
   },
   "outputs": [],
   "source": [
    "def get_model_results(ldamodel, corpus, dictionary):\n",
    "    \"\"\" Create doc-topic probabilities table and visualization for the LDA model\n",
    "\n",
    "    \"\"\"  \n",
    "    vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "    transformed = ldamodel.get_document_topics(corpus)\n",
    "    df = pd.DataFrame.from_records([{v:k for v, k in row} for row in transformed])\n",
    "    return vis, df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "118275936bf3d97327624189cea18b8b5d3ac4e2"
   },
   "outputs": [],
   "source": [
    "def get_model_wordcloud(ldamodel):\n",
    "    \"\"\" Create a Word Cloud for each topic of the LDA model\n",
    "\n",
    "    \"\"\"  \n",
    "    plot_cols = 3\n",
    "    plot_rows = math.ceil(num_topics / 3)\n",
    "    axisNum = 0\n",
    "    plt.figure(figsize=(5*plot_cols, 3*plot_rows))\n",
    "    for topicID in range(ldamodel.state.get_lambda().shape[0]):\n",
    "        #gather most relevant terms for the given topic\n",
    "        topics_terms = ldamodel.state.get_lambda()\n",
    "        tmpDict = {}\n",
    "        for i in range(1, len(topics_terms[0])):\n",
    "            tmpDict[ldamodel.id2word[i]]=topics_terms[topicID,i]\n",
    "\n",
    "        # draw the wordcloud\n",
    "        wordcloud = WordCloud( margin=0,max_words=20 ).generate_from_frequencies(tmpDict)\n",
    "        axisNum += 1\n",
    "        ax = plt.subplot(plot_rows, plot_cols, axisNum)\n",
    "\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        title = topicID\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "        plt.margins(x=0, y=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb9b52b789b3a47d5acf04bfa4f9831809ec1c04"
   },
   "outputs": [],
   "source": [
    "def topic_query(data, query):\n",
    "    \"\"\" Get Documents matching the query with the doc-topic probabilities\n",
    "\n",
    "    \"\"\"  \n",
    "    result = data\n",
    "    result['sort'] = 0\n",
    "    for topic in query:\n",
    "        result = result[result[topic] >= query[topic]]\n",
    "        result['sort'] += result[topic]\n",
    "    result = result.sort_values(['sort'], ascending=False)\n",
    "    result = result.drop('sort', axis=1)\n",
    "    result = result.head(5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3b3157ba4d863b69eae25ef5fe0894ab9695845"
   },
   "outputs": [],
   "source": [
    "def get_text_topics(text, top=20):\n",
    "    \"\"\" Get the topics probabilities for a text and highlight relevant words\n",
    "\n",
    "    \"\"\"    \n",
    "    def token_topic(token):\n",
    "        return topic_words.get(token, -1)\n",
    "    \n",
    "    colors = ['\\033[46m', '\\033[45m', '\\033[44m', '\\033[43m', '\\033[42m', '\\033[41m', '\\033[47m']    \n",
    "    nlp_tokens = nlp_preprocessing([text])\n",
    "\n",
    "    bow_text = [lda_dic.doc2bow(doc) for doc in nlp_tokens]\n",
    "    bow_text = lda_tfidf[bow_text]\n",
    "    topic_text = lda_model.get_document_topics(bow_text)\n",
    "    topic_text = pd.DataFrame.from_records([{v:k for v, k in row} for row in topic_text])\n",
    "    \n",
    "    topic_labeled = 0\n",
    "    for topic in topic_text:\n",
    "        print(colors[topic_labeled % len(colors)]+'Topic '+str(topic)+':', '{0:.2%}'.format(topic_text[topic].values[0])+'\\033[0m')\n",
    "        topic_labeled += 1\n",
    "    print('')\n",
    "    topic_words = []\n",
    "    topic_labeled = 0\n",
    "    for topic in topic_text.columns.values:\n",
    "        topic_terms = lda_model.get_topic_terms(topic, top)\n",
    "        topic_words = topic_words+[[topic_labeled, lda_dic[pair[0]], pair[1]] for pair in topic_terms]\n",
    "        topic_labeled += 1\n",
    "    topic_words = pd.DataFrame(topic_words, columns=['topic', 'word', 'value']).pivot(index='word', columns='topic', values='value').idxmax(axis=1)\n",
    "    nlp_doc = nlp(text)\n",
    "    text_highlight = ''.join([x.string if token_topic(x.lemma_.lower()) <0  else colors[token_topic(x.lemma_.lower()) % len(colors)] + x.string + '\\033[0m' for x in nlp_doc])\n",
    "    print(text_highlight) \n",
    "\n",
    "    # Plot Pie chart\n",
    "    plt_data = topic_text\n",
    "    plt_data.columns = ['Topic '+str(c) for c in plt_data.columns]\n",
    "    plt_data['Others'] = 1-plt_data.sum(axis=1)\n",
    "    plt_data = plt_data.T\n",
    "    plt_data.plot(kind='pie', y=0, autopct='%.2f')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.title('Topics Probabilities')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a15892694b985e8d7859a5984ecfe2a91634b6c1"
   },
   "source": [
    "## Model <a id=\"lda_model\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90bb54123aea8ef9384f9b0664afd4a56a0d6478"
   },
   "outputs": [],
   "source": [
    "lda_tokens = questions['nlp_tokens']\n",
    "# Gensim Dictionary\n",
    "lda_dic = gensim.corpora.Dictionary(lda_tokens)\n",
    "lda_dic.filter_extremes(no_below=extremes_no_below, no_above=extremes_no_above, keep_n=extremes_keep_n)\n",
    "lda_corpus = [lda_dic.doc2bow(doc) for doc in lda_tokens]\n",
    "\n",
    "lda_tfidf = gensim.models.TfidfModel(lda_corpus)\n",
    "lda_corpus = lda_tfidf[lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6cd15773628452a2485f9f0ca6891bcfc95735a"
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(lda_corpus, num_topics=num_topics, \n",
    "                                            id2word = lda_dic, passes=passes,\n",
    "                                            chunksize=chunksize,update_every=0, \n",
    "                                            alpha=alpha, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b153a6d1aab72ceea7cd059ea2dd8656f154839b"
   },
   "source": [
    "## Topics  <a id=\"lda_topics\"></a> \n",
    "Each wordcloud shows a topic and the top words who define the topic. \n",
    "Here some examples:    \n",
    "Topic 0 is for teacher (*teacher, teaching, education, ...*)  \n",
    "Topic 1 is for designer (*design, video, graphic, art, ...*)  \n",
    "Topic 4 is for veterinary (*veterainary, vet, animal, ...*) but seems to be for actors to (*film, theatre, music, singer, ...*)  \n",
    "Topic 9 is for health (*medicine, doctor, dental, ...*)  \n",
    "Topic 13 is for engineers (*engineering, mechanical, aerospace, electrical, ...*)  \n",
    "Topic 17 is for sport (*sport, athlet, basketball, ...*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72d42361a13378fadfaccb523c299495c56a8aef"
   },
   "outputs": [],
   "source": [
    "get_model_wordcloud(lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f13e31b8e45d9a1382394856cfc7a8e7278285aa"
   },
   "source": [
    "## Interactive Visualization  \n",
    "*lda_vis* is a interactive visualization for topic model. But it makes some problems with the sceen width on kaggle, so I commented it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3859ee0e992c02ac6f672e9ac0213add425fc9c"
   },
   "outputs": [],
   "source": [
    "lda_vis, lda_result = get_model_results(lda_model, lda_corpus, lda_dic)\n",
    "#lda_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b90d4f83a55c10f4d1b688b674a183c212783ba"
   },
   "source": [
    "## Document-Topic Probabilities <a id=\"lda_doc_topic_prob\"></a> \n",
    "Here are the topic probabilites for the first five questions.  \n",
    "Topics with *NaN* values for these five question were deleted.  \n",
    "If a topic probabilites is under a give threshold it gets automaticaly a *NaN* value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "318227c84748729db71a7a3e9a07edc2e1c98497"
   },
   "outputs": [],
   "source": [
    "lda_questions = questions[['questions_id', 'questions_title', 'questions_body']]\n",
    "lda_questions = pd.concat([lda_questions, lda_result.add_prefix('Topic_')], axis=1)\n",
    "lda_questions.head(5).dropna(axis=1, how='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "057926656658ac7d8fce6e80fb9ca81be162f660"
   },
   "source": [
    "## Example 1  <a id=\"lda_example1\"></a> \n",
    "The example with the data science text was assigned to topic 3 with 87%.  \n",
    "The highlighted text are words, who define the topic.  \n",
    "A look at the previously created wordcloud shows that topic 3 is a mix of *math* and *computer science*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c1d7c9bc7eeceda27ab5fbf16637a4082dc46ba"
   },
   "outputs": [],
   "source": [
    "query_text = 'I will finish my college next year and would like to start a career as a Data Scientist. \\n'\\\n",
    "            +'What is the best way to become a good Data Scientist? #data-science'\n",
    "get_text_topics(query_text, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66c068009b4908e435604b437da6f9dbe23ed845"
   },
   "source": [
    "## Example 2  <a id=\"lda_example2\"></a>\n",
    "Now I would like to make a query, which gives me back documents with the topic *veterinary (**Topic 4**)* and *health (**Topic 9**)*.  \n",
    "The first two questions are about descision to begin the career as a veterinarian (*Topic 4*) or in another medical field (*Topic 9*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d3b1ba5573567fdfac96cb0bd0fe28702602d2fa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query = {'Topic_4':0.4, 'Topic_9':0.4}\n",
    "topic_query(lda_questions, query).dropna(axis=1, how='all').head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66ab0c9fafa3ff8fa938e82a96a1c50ce28e2ef7"
   },
   "outputs": [],
   "source": [
    "get_text_topics(questions['questions_full_text'][20658], 50)\n",
    "print()\n",
    "get_text_topics(questions['questions_full_text'][3075], 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0bcbc76b02684d5e61248d025799026b071d26ad"
   },
   "source": [
    "# 7. Next steps <a id=\"next_steps\"></a>  \n",
    "* Additional features for the recommendation.  \n",
    "* Evaluation of the LDA model (parameters, randscore)  \n",
    "* Professionals scoring  \n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3346123414d885f11a9d2533e8c1fe8315e42610"
   },
   "source": [
    "# 8. Version control <a id=\"version_control\"></a>  \n",
    "10.03.2019:  \n",
    "* Update dependency graph\n",
    "* Code improvement\n",
    "\n",
    "09.03.2019:  \n",
    "* Recommendation: Scoring and Visualization\n",
    "* LDA Pie charts for topic probabilities\n",
    "\n",
    "08.03.2019:  \n",
    "* Feature Extraction\n",
    "* Some code optimization\n",
    "\n",
    "07.03.2019:  \n",
    "* Dependency graph\n",
    "* Change workflow section\n",
    "\n",
    "06.03.2019:  \n",
    "* Workflow diagram  \n",
    "* Bubble charts for tags  \n",
    "\n",
    "05.03.2019:  \n",
    "* Initial version  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa047e14918b401bde0df3708f8642212da09bd0"
   },
   "source": [
    "**I'm on vacation for the next days and unfortunately I won't be able to do any updates.  \n",
    "But you are welcome to leave a comment and upvote if you find this kernel useful.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0137dbd02f26ba096e7b0a7d0105d005ae9fb3c8"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
